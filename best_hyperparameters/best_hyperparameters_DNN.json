{
    "n_layers": 3,
    "nodes_l0": 63,
    "nodes_l1": 60,
    "nodes_l2": 63,
    "dropout_l0": 0.13776710985647656,
    "dropout_l1": 0.2703074160473001,
    "dropout_l2": 0.24809947846919952,
    "l2_l0": 0.0069428039254270605,
    "l2_l1": 0.0002064821181469899,
    "l2_l2": 0.004286022879399688,
    "learning_rate": 0.00012431705552158061,
    "batch_size": 64,
    "epochs": 43
}